{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "182ea743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Reshape, UpSampling1D\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import csv\n",
    "import librosa\n",
    "import librosa.display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy.io import wavfile\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46bce9d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "seed = 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f94cb758",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy_and_loss(history_df, name, idx):\n",
    "    # Plot and save accuraccy\n",
    "    plt.plot(history_df['accuracy'])\n",
    "    plt.plot(history_df['val_accuracy'])\n",
    "    plt.title(f'{name}: accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    path = f'train_history/{name}/{idx}_accuracy.png'\n",
    "    plt.savefig(path)\n",
    "    print(f'Accuracy plot is saved to: {path}')\n",
    "    plt.close()\n",
    "    # Plot and save loss\n",
    "    plt.figure()\n",
    "    plt.plot(history_df['loss'])\n",
    "    plt.plot(history_df['val_loss'])\n",
    "    plt.title(f'{name}: loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper right')\n",
    "    path = f'train_history/{name}/{idx}_loss.png'\n",
    "    plt.savefig(path)\n",
    "    print(f'Loss plot is saved to: {path}')\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(name, idx):\n",
    "    model = keras.models.load_model('train_history/'+name+'/.keras')\n",
    "    y_pred = np.argmax(model.predict(X_val), axis=-1)\n",
    "\n",
    "    cm = confusion_matrix(y_val.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=range(0, 10), yticklabels=range(0, 10))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title(f'{name} Confusion Matrix. Accuracy: {accuracy * 100:.2f}%')\n",
    "    path = f'train_history/{name}/{idx}_confusion_matrix.png'\n",
    "    plt.savefig(path)\n",
    "    print(f'Confusion matrix is saved to: {path}')\n",
    "    plt.close()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7d9f5a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XXX\\AppData\\Local\\Temp\\ipykernel_14200\\1536078993.py:17: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  noise_sample_rate, noise_sample = wavfile.read(os.path.join('../data/train/audio/_background_noise_', noises))\n"
     ]
    }
   ],
   "source": [
    "noise_coef = 0.1\n",
    "silence_coef = 0.1\n",
    "drop_out_rate = 0.5\n",
    "num_epochs = 10\n",
    "num_silences = 1700\n",
    "audio_length = 16000\n",
    "sample_rate = 16000\n",
    "classes = ['down', 'go', 'left', 'no', 'off', 'on', 'right', 'silence', 'stop', 'unknown', 'up', 'yes']\n",
    "\n",
    "# To hold waves as xs and labels as y of training\n",
    "train_samples = []\n",
    "train_labels = []\n",
    "noise_samples = []\n",
    "for noises in os.listdir('../data/train/audio/_background_noise_'):\n",
    "    if noises[-4:] != '.wav':\n",
    "        continue\n",
    "    noise_sample_rate, noise_sample = wavfile.read(os.path.join('../data/train/audio/_background_noise_', noises))\n",
    "    #noise_resampled = librosa.resample(noise_sample.astype(np.float), orig_sr=noise_sample_rate, target_sr=sample_rate)\n",
    "    noise_resampled = noise_sample\n",
    "    noise_samples.append(noise_resampled)\n",
    "\n",
    "# Loading all the waves and labels\n",
    "for trains in os.listdir('../data/train/audio'):\n",
    "    if trains == '_background_noise_':\n",
    "        continue\n",
    "    for wav_file in os.listdir(os.path.join('../data/train/audio', trains)):\n",
    "        if wav_file[-4:] != '.wav':\n",
    "            continue\n",
    "        sample_rate_init, sample = wavfile.read(os.path.join(os.path.join('../data/train/audio', trains), wav_file))\n",
    "        signal_resampled = sample\n",
    "        #signal_resampled = librosa.resample(sample.astype(np.float), orig_sr=sample_rate_init, target_sr=sample_rate)\n",
    "        if len(signal_resampled) != audio_length:\n",
    "            zeros_needed = audio_length - len(signal_resampled)\n",
    "            signal_resampled = np.append(signal_resampled, np.zeros((zeros_needed))) # if the length is not right just pad with zeros\n",
    "        #adding noise to the samples\n",
    "        nois_type_indx = np.random.randint(0, len(noise_samples), 1)\n",
    "        nois_sample_indx = np.random.randint(0, len(noise_samples[int(nois_type_indx)]) - audio_length, 1)\n",
    "        sample_noise = signal_resampled + noise_coef * noise_samples[int(nois_type_indx)][int(nois_sample_indx):int(nois_sample_indx) + audio_length]\n",
    "        train_samples.append(sample_noise)\n",
    "        if trains not in ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']:\n",
    "            train_labels.append('unknown')\n",
    "        else:\n",
    "            train_labels.append(trains)\n",
    "\n",
    "# adding portions of weakened background noise as silence\n",
    "for num_sil in range(num_silences):\n",
    "    silence_type_indx = np.random.randint(0, len(noise_samples), 1)\n",
    "    silence_sample_indx = np.random.randint(0, len(noise_samples[int(silence_type_indx)]) - audio_length, 1)\n",
    "    sample_silence = silence_coef * noise_samples[int(silence_type_indx)][int(silence_sample_indx):int(silence_sample_indx) + audio_length]\n",
    "    train_labels.append('silence')\n",
    "    train_samples.append(sample_silence)\n",
    "    \n",
    "# converting labels from text to int\n",
    "le = LabelEncoder()\n",
    "y_all = le.fit_transform(train_labels)\n",
    "y = keras.utils.to_categorical(y_all, len(le.classes_))\n",
    "# To make sure all arrays are having a defined size\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_samples, y, test_size=0.2, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bc61390",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (len(X_train), len(X_train[0]), 1))\n",
    "X_val = np.reshape(X_val, (len(X_val), len(X_val[0]), 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35427a",
   "metadata": {},
   "source": [
    "## Test 1: ConvS2S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2b6ffa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1661/1661 [==============================] - 1931s 1s/step - loss: 24799.5273 - accuracy: 0.6828 - val_loss: 1.2038 - val_accuracy: 0.7870\n",
      "Epoch 2/10\n",
      "1661/1661 [==============================] - 1894s 1s/step - loss: 14.7909 - accuracy: 0.8724 - val_loss: 0.0509 - val_accuracy: 0.9369\n",
      "Epoch 3/10\n",
      "1661/1661 [==============================] - 2590s 2s/step - loss: 0.0511 - accuracy: 0.9362 - val_loss: 0.0505 - val_accuracy: 0.9369\n",
      "Epoch 4/10\n",
      "1661/1661 [==============================] - 2106s 1s/step - loss: 0.0508 - accuracy: 0.9362 - val_loss: 0.0503 - val_accuracy: 0.9369\n",
      "Epoch 5/10\n",
      "1661/1661 [==============================] - 1982s 1s/step - loss: 0.0506 - accuracy: 0.9362 - val_loss: 0.0501 - val_accuracy: 0.9369\n",
      "Epoch 6/10\n",
      "1448/1661 [=========================>....] - ETA: 4:20 - loss: 0.0506 - accuracy: 0.9361"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Graph execution error:\n\nDetected at node 'gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad' defined at (most recent call last):\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\XXX\\AppData\\Local\\Temp\\ipykernel_12820\\3048059351.py\", line 40, in <module>\n      history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad'\nOOM when allocating tensor with shape[32,16000,1,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1852]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12820\\3048059351.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Train the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 40\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvs2s_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;31m# To get the full stack trace, call:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m             \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     50\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     54\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: Graph execution error:\n\nDetected at node 'gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad' defined at (most recent call last):\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\traitlets\\config\\application.py\", line 846, in launch_instance\n      app.start()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 712, in start\n      self.io_loop.start()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 199, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 510, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 499, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 406, in dispatch_shell\n      await result\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 730, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 390, in do_execute\n      res = shell.run_cell(code, store_history=store_history, silent=silent)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 528, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2914, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2960, in _run_cell\n      return runner(coro)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 78, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3185, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3377, in run_ast_nodes\n      if (await self.run_code(code, result,  async_=asy)):\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3457, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\XXX\\AppData\\Local\\Temp\\ipykernel_12820\\3048059351.py\", line 40, in <module>\n      history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1564, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1160, in train_function\n      return step_function(self, iterator)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1146, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 1135, in run_step\n      outputs = model.train_step(data)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\", line 997, in train_step\n      self.optimizer.minimize(loss, self.trainable_variables, tape=tape)\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 576, in minimize\n      grads_and_vars = self._compute_gradients(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 634, in _compute_gradients\n      grads_and_vars = self._get_gradients(\n    File \"C:\\Users\\XXX\\anaconda3\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\optimizer_v2.py\", line 510, in _get_gradients\n      grads = tape.gradient(loss, var_list, grad_loss)\nNode: 'gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad'\nOOM when allocating tensor with shape[32,16000,1,64] and type float on /job:localhost/replica:0/task:0/device:CPU:0 by allocator cpu\n\t [[{{node gradient_tape/model/max_pooling1d/MaxPool/MaxPoolGrad}}]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info. This isn't available when running in Eager mode.\n [Op:__inference_train_function_1852]"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(seed)\n",
    "accuracy = []\n",
    "batch_size=32\n",
    "n_epochs=10\n",
    "optimizer='adam'\n",
    "loss='mse'\n",
    "\n",
    "# Define ConvS2S model architecture\n",
    "def build_convs2s(input_shape, output_length, num_filters=64, kernel_size=3):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    flatten = Flatten()(maxpool2)\n",
    "    encoder_output = Dense(num_filters, activation='relu')(flatten)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Dense(output_length*num_filters, activation='relu')(encoder_output)\n",
    "    reshape = Reshape((output_length, num_filters))(decoder_input)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(reshape)\n",
    "    upsample1 = UpSampling1D(size=2)(conv3)\n",
    "    conv4 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(upsample1)\n",
    "    upsample2 = UpSampling1D(size=2)(conv4)\n",
    "    decoder_output = Conv1D(1, 37, activation='linear')(upsample2)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (16000, 1)\n",
    "output_length = 12\n",
    "convs2s_model = build_convs2s(input_shape, output_length)\n",
    "\n",
    "# Train the model\n",
    "history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b215b372",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12820\\2385869370.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mconvs2s_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'train_history/{name}.keras'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mhistory_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mhist_csv_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'train_history/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mname\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'/'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'_history.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhist_csv_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'w'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "name = 'convs2s_1' \n",
    "\n",
    "convs2s_model.save(f'train_history/{name}.keras')\n",
    "history_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'train_history/' + name + '/' + str(idx) + '_history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    history_df.to_csv(f)\n",
    "plot_accuracy_and_loss(history_df, name, str(idx))\n",
    "curr_accuracy = plot_confusion_matrix(name, str(idx))\n",
    "accuracy.append(curr_accuracy)\n",
    "print(f'Attempt accuracy: {curr_accuracy * 100:.2f}%')\n",
    "    \n",
    "accuracy_df = pd.DataFrame(data=accuracy, columns = ['accuracy'])\n",
    "accuracy_csv_file = 'train_history/' + name + '/accuracy.csv'\n",
    "with open(accuracy_csv_file, mode='w') as f:\n",
    "    accuracy_df.to_csv(f)\n",
    "print(f'Attempts accuracy is saved to {accuracy_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bc91499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1661/1661 [==============================] - 1878s 1s/step - loss: 18029.3086 - accuracy: 0.5933 - val_loss: 7.8794 - val_accuracy: 0.4969\n",
      "Epoch 2/5\n",
      "1661/1661 [==============================] - 1802s 1s/step - loss: 35.0591 - accuracy: 0.6632 - val_loss: 1.9260 - val_accuracy: 0.6587\n",
      "Epoch 3/5\n",
      "1661/1661 [==============================] - 1795s 1s/step - loss: 19.4979 - accuracy: 0.6989 - val_loss: 1.3874 - val_accuracy: 0.5069\n",
      "Epoch 4/5\n",
      "1661/1661 [==============================] - 1802s 1s/step - loss: 14.2842 - accuracy: 0.7674 - val_loss: 0.2318 - val_accuracy: 0.8878\n",
      "Epoch 5/5\n",
      "1661/1661 [==============================] - 1816s 1s/step - loss: 0.1838 - accuracy: 0.9177 - val_loss: 0.0659 - val_accuracy: 0.9222\n"
     ]
    }
   ],
   "source": [
    "name = 'convs2s_2'\n",
    "keras.utils.set_random_seed(seed)\n",
    "accuracy = []\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "optimizer='adamax'\n",
    "loss='mse'\n",
    "\n",
    "# Define ConvS2S model architecture\n",
    "def build_convs2s(input_shape, output_length, num_filters=64, kernel_size=3):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    flatten = Flatten()(maxpool2)\n",
    "    encoder_output = Dense(num_filters, activation='relu')(flatten)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Dense(output_length*num_filters, activation='relu')(encoder_output)\n",
    "    reshape = Reshape((output_length, num_filters))(decoder_input)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(reshape)\n",
    "    upsample1 = UpSampling1D(size=2)(conv3)\n",
    "    conv4 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(upsample1)\n",
    "    upsample2 = UpSampling1D(size=2)(conv4)\n",
    "    decoder_output = Conv1D(1, 37, activation='linear')(upsample2)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (16000, 1)\n",
    "output_length = 12\n",
    "convs2s_model = build_convs2s(input_shape, output_length)\n",
    "\n",
    "# Train the model\n",
    "history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57c1ca74",
   "metadata": {},
   "outputs": [],
   "source": [
    "convs2s_model = load_model('train_history/convs2s_2.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c4c4b256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy plot is saved to: train_history/convs2s_5/1_accuracy.png\n",
      "Loss plot is saved to: train_history/convs2s_5/1_loss.png\n",
      "416/416 [==============================] - 139s 332ms/step\n",
      "Accuracy: 0.00%\n",
      "Confusion matrix is saved to: train_history/convs2s_5/1_confusion_matrix.png\n",
      "Attempt accuracy: 0.00%\n",
      "Attempts accuracy is saved to train_history/convs2s_5/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "convs2s_model.save(f'train_history/{name}/.keras')\n",
    "history_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'train_history/' + name + '/' + 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    history_df.to_csv(f)\n",
    "plot_accuracy_and_loss(history_df, name, str(1))\n",
    "curr_accuracy = plot_confusion_matrix(name, str(1))\n",
    "accuracy.append(curr_accuracy)\n",
    "print(f'Attempt accuracy: {curr_accuracy * 100:.2f}%')\n",
    "    \n",
    "accuracy_df = pd.DataFrame(data=accuracy, columns = ['accuracy'])\n",
    "accuracy_csv_file = 'train_history/' + name + '/accuracy.csv'\n",
    "with open(accuracy_csv_file, mode='w') as f:\n",
    "    accuracy_df.to_csv(f)\n",
    "print(f'Attempts accuracy is saved to {accuracy_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d98d8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1661/1661 [==============================] - 1779s 1s/step - loss: nan - accuracy: 0.9160 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 2/5\n",
      "1661/1661 [==============================] - 1742s 1s/step - loss: nan - accuracy: 0.9167 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 3/5\n",
      "1661/1661 [==============================] - 1729s 1s/step - loss: nan - accuracy: 0.9167 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 4/5\n",
      "1661/1661 [==============================] - 1720s 1s/step - loss: nan - accuracy: 0.9167 - val_loss: nan - val_accuracy: 0.9167\n",
      "Epoch 5/5\n",
      "1661/1661 [==============================] - 1756s 1s/step - loss: nan - accuracy: 0.9167 - val_loss: nan - val_accuracy: 0.9167\n"
     ]
    }
   ],
   "source": [
    "name = 'convs2s_3'\n",
    "keras.utils.set_random_seed(seed)\n",
    "accuracy = []\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "optimizer='sgd'\n",
    "loss='mse'\n",
    "\n",
    "# Define ConvS2S model architecture\n",
    "def build_convs2s(input_shape, output_length, num_filters=64, kernel_size=3):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    flatten = Flatten()(maxpool2)\n",
    "    encoder_output = Dense(num_filters, activation='relu')(flatten)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Dense(output_length*num_filters, activation='relu')(encoder_output)\n",
    "    reshape = Reshape((output_length, num_filters))(decoder_input)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(reshape)\n",
    "    upsample1 = UpSampling1D(size=2)(conv3)\n",
    "    conv4 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(upsample1)\n",
    "    upsample2 = UpSampling1D(size=2)(conv4)\n",
    "    decoder_output = Conv1D(1, 37, activation='linear')(upsample2)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (16000, 1)\n",
    "output_length = 12\n",
    "convs2s_model = build_convs2s(input_shape, output_length)\n",
    "\n",
    "# Train the model\n",
    "history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2b7fe7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy plot is saved to: train_history/convs2s_3/1_accuracy.png\n",
      "Loss plot is saved to: train_history/convs2s_3/1_loss.png\n",
      "416/416 [==============================] - 120s 287ms/step\n",
      "Accuracy: 0.00%\n",
      "Confusion matrix is saved to: train_history/convs2s_3/1_confusion_matrix.png\n",
      "Attempt accuracy: 0.00%\n",
      "Attempts accuracy is saved to train_history/convs2s_3/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "convs2s_model.save(f'train_history/{name}/.keras')\n",
    "history_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'train_history/' + name + '/' + 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    history_df.to_csv(f)\n",
    "plot_accuracy_and_loss(history_df, name, str(1))\n",
    "curr_accuracy = plot_confusion_matrix(name, str(1))\n",
    "accuracy.append(curr_accuracy)\n",
    "print(f'Attempt accuracy: {curr_accuracy * 100:.2f}%')\n",
    "    \n",
    "accuracy_df = pd.DataFrame(data=accuracy, columns = ['accuracy'])\n",
    "accuracy_csv_file = 'train_history/' + name + '/accuracy.csv'\n",
    "with open(accuracy_csv_file, mode='w') as f:\n",
    "    accuracy_df.to_csv(f)\n",
    "print(f'Attempts accuracy is saved to {accuracy_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e22899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1661/1661 [==============================] - 1857s 1s/step - loss: 9.4958 - accuracy: 0.5586 - val_loss: 13.9840 - val_accuracy: 0.6054\n",
      "Epoch 2/5\n",
      "1661/1661 [==============================] - 1826s 1s/step - loss: 2.3145 - accuracy: 0.6055 - val_loss: 2.1341 - val_accuracy: 0.6054\n",
      "Epoch 3/5\n",
      "1661/1661 [==============================] - 2071s 1s/step - loss: 2.1400 - accuracy: 0.6055 - val_loss: 2.1341 - val_accuracy: 0.6054\n",
      "Epoch 4/5\n",
      "1661/1661 [==============================] - 1926s 1s/step - loss: 2.1400 - accuracy: 0.6055 - val_loss: 2.1341 - val_accuracy: 0.6054\n",
      "Epoch 5/5\n",
      "1661/1661 [==============================] - 1844s 1s/step - loss: 2.1400 - accuracy: 0.6055 - val_loss: 2.1341 - val_accuracy: 0.6054\n"
     ]
    }
   ],
   "source": [
    "keras.utils.set_random_seed(seed)\n",
    "accuracy = []\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "optimizer='adam'\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "# Define ConvS2S model architecture\n",
    "def build_convs2s(input_shape, output_length, num_filters=64, kernel_size=3):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    flatten = Flatten()(maxpool2)\n",
    "    encoder_output = Dense(num_filters, activation='relu')(flatten)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Dense(output_length*num_filters, activation='relu')(encoder_output)\n",
    "    reshape = Reshape((output_length, num_filters))(decoder_input)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(reshape)\n",
    "    upsample1 = UpSampling1D(size=2)(conv3)\n",
    "    conv4 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(upsample1)\n",
    "    upsample2 = UpSampling1D(size=2)(conv4)\n",
    "    decoder_output = Conv1D(1, 37, activation='linear')(upsample2)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (16000, 1)\n",
    "output_length = 12\n",
    "convs2s_model = build_convs2s(input_shape, output_length)\n",
    "\n",
    "# Train the model\n",
    "history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "574d63fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy plot is saved to: train_history/convs2s_4/1_accuracy.png\n",
      "Loss plot is saved to: train_history/convs2s_4/1_loss.png\n",
      "416/416 [==============================] - 130s 310ms/step\n",
      "Accuracy: 0.00%\n",
      "Confusion matrix is saved to: train_history/convs2s_4/1_confusion_matrix.png\n",
      "Attempt accuracy: 0.00%\n",
      "Attempts accuracy is saved to train_history/convs2s_4/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "name = 'convs2s_4'\n",
    "\n",
    "convs2s_model.save(f'train_history/{name}/.keras')\n",
    "history_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'train_history/' + name + '/' + 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    history_df.to_csv(f)\n",
    "plot_accuracy_and_loss(history_df, name, str(1))\n",
    "curr_accuracy = plot_confusion_matrix(name, str(1))\n",
    "accuracy.append(curr_accuracy)\n",
    "print(f'Attempt accuracy: {curr_accuracy * 100:.2f}%')\n",
    "    \n",
    "accuracy_df = pd.DataFrame(data=accuracy, columns = ['accuracy'])\n",
    "accuracy_csv_file = 'train_history/' + name + '/accuracy.csv'\n",
    "with open(accuracy_csv_file, mode='w') as f:\n",
    "    accuracy_df.to_csv(f)\n",
    "print(f'Attempts accuracy is saved to {accuracy_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4edf8544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1661/1661 [==============================] - 2022s 1s/step - loss: 1826488.1250 - accuracy: 0.9172 - val_loss: 0.0504 - val_accuracy: 0.9369\n",
      "Epoch 2/5\n",
      "1661/1661 [==============================] - 3166s 2s/step - loss: 0.0508 - accuracy: 0.9362 - val_loss: 0.0506 - val_accuracy: 0.9369\n",
      "Epoch 3/5\n",
      "1661/1661 [==============================] - 2122s 1s/step - loss: 0.0507 - accuracy: 0.9362 - val_loss: 0.0501 - val_accuracy: 0.9369\n",
      "Epoch 4/5\n",
      "1661/1661 [==============================] - 1997s 1s/step - loss: 0.0506 - accuracy: 0.9362 - val_loss: 0.0502 - val_accuracy: 0.9369\n",
      "Epoch 5/5\n",
      "1661/1661 [==============================] - 2508s 2s/step - loss: 0.0506 - accuracy: 0.9361 - val_loss: 0.0502 - val_accuracy: 0.9369\n"
     ]
    }
   ],
   "source": [
    "name = 'convs2s_5'\n",
    "keras.utils.set_random_seed(seed)\n",
    "accuracy = []\n",
    "batch_size=32\n",
    "n_epochs=5\n",
    "optimizer='RMSprop'\n",
    "loss='mse'\n",
    "\n",
    "# Define ConvS2S model architecture\n",
    "def build_convs2s(input_shape, output_length, num_filters=64, kernel_size=3):\n",
    "    # Encoder\n",
    "    inputs = Input(shape=input_shape)\n",
    "    conv1 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(inputs)\n",
    "    maxpool1 = MaxPooling1D(pool_size=2)(conv1)\n",
    "    conv2 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(maxpool1)\n",
    "    maxpool2 = MaxPooling1D(pool_size=2)(conv2)\n",
    "    flatten = Flatten()(maxpool2)\n",
    "    encoder_output = Dense(num_filters, activation='relu')(flatten)\n",
    "\n",
    "    # Decoder\n",
    "    decoder_input = Dense(output_length*num_filters, activation='relu')(encoder_output)\n",
    "    reshape = Reshape((output_length, num_filters))(decoder_input)\n",
    "    conv3 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(reshape)\n",
    "    upsample1 = UpSampling1D(size=2)(conv3)\n",
    "    conv4 = Conv1D(num_filters, kernel_size, activation='relu', padding='same')(upsample1)\n",
    "    upsample2 = UpSampling1D(size=2)(conv4)\n",
    "    decoder_output = Conv1D(1, 37, activation='linear')(upsample2)\n",
    "\n",
    "    # Build and compile model\n",
    "    model = Model(inputs=inputs, outputs=decoder_output)\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "# Usage:\n",
    "input_shape = (16000, 1)\n",
    "output_length = 12\n",
    "convs2s_model = build_convs2s(input_shape, output_length)\n",
    "\n",
    "# Train the model\n",
    "history = convs2s_model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dce34878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy plot is saved to: train_history/convs2s_5/1_accuracy.png\n",
      "Loss plot is saved to: train_history/convs2s_5/1_loss.png\n",
      "416/416 [==============================] - 121s 287ms/step\n",
      "Accuracy: 0.00%\n",
      "Confusion matrix is saved to: train_history/convs2s_5/1_confusion_matrix.png\n",
      "Attempt accuracy: 0.00%\n",
      "Attempts accuracy is saved to train_history/convs2s_5/accuracy.csv\n"
     ]
    }
   ],
   "source": [
    "convs2s_model.save(f'train_history/{name}/.keras')\n",
    "history_df = pd.DataFrame(history.history) \n",
    "hist_csv_file = 'train_history/' + name + '/' + 'history.csv'\n",
    "with open(hist_csv_file, mode='w') as f:\n",
    "    history_df.to_csv(f)\n",
    "plot_accuracy_and_loss(history_df, name, str(1))\n",
    "curr_accuracy = plot_confusion_matrix(name, str(1))\n",
    "accuracy.append(curr_accuracy)\n",
    "print(f'Attempt accuracy: {curr_accuracy * 100:.2f}%')\n",
    "    \n",
    "accuracy_df = pd.DataFrame(data=accuracy, columns = ['accuracy'])\n",
    "accuracy_csv_file = 'train_history/' + name + '/accuracy.csv'\n",
    "with open(accuracy_csv_file, mode='w') as f:\n",
    "    accuracy_df.to_csv(f)\n",
    "print(f'Attempts accuracy is saved to {accuracy_csv_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ec5458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
